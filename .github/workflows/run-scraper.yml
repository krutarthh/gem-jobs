# Run the Gold Gem Jobs scraper every 15 minutes.
# Persists SQLite DB between runs via cache so only new jobs trigger Discord alerts.

name: Run scraper every 15 min

on:
  schedule:
    - cron: '*/15 * * * *'
  workflow_dispatch:

# Prevent overlapping runs: only one at a time; new run cancels the previous
concurrency:
  group: scraper
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Single stable key: restore and save the same cache so we update it each run instead of creating a new one.
      # Concurrency (cancel-in-progress) ensures only one run at a time, so no conflict on save.
      - name: Restore job DB cache
        id: restore-cache
        uses: actions/cache/restore@v4
        with:
          path: data
          key: gold-gem-jobs-db

      - name: Log cache restore result
        run: |
          echo "::group::Cache restore diagnostics"
          echo "cache-hit: ${{ steps.restore-cache.outputs.cache-hit }}"
          echo "cache-primary-key: ${{ steps.restore-cache.outputs.cache-primary-key }}"
          echo "cache-matched-key: ${{ steps.restore-cache.outputs.cache-matched-key }}"
          if [ -f data/jobs.db ]; then
            echo "jobs.db exists after restore; size: $(du -h data/jobs.db | cut -f1)"
            echo "job count: $(sqlite3 data/jobs.db 'SELECT COUNT(*) FROM jobs;' 2>/dev/null || echo '?')"
          else
            echo "jobs.db missing after restore (first run or cache miss)"
          fi
          echo "::endgroup::"

      - name: Create data dir if missing
        run: mkdir -p data

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run scraper
        id: scrape
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: python -m src.main

      # Keep DB small: prune old jobs (90 days), then compact. Smaller DB = fewer random cache save failures.
      - name: Prune and compact DB before cache
        if: success()
        run: |
          if [ -f data/jobs.db ]; then
            sqlite3 data/jobs.db "DELETE FROM jobs WHERE last_seen_at < datetime('now', '-90 days');" 2>/dev/null || true
            sqlite3 data/jobs.db "VACUUM;" 2>/dev/null || true
          fi

      - name: Log state before cache save
        if: success()
        run: |
          echo "::group::Pre-save diagnostics"
          echo "data/ contents:"
          ls -la data/ 2>/dev/null || true
          echo "data/ size: $(du -sh data/ 2>/dev/null | cut -f1 || echo 'N/A')"
          if [ -f data/jobs.db ]; then
            echo "jobs.db size: $(du -h data/jobs.db | cut -f1)"
            echo "jobs count: $(sqlite3 data/jobs.db 'SELECT COUNT(*) FROM jobs;' 2>/dev/null || echo '?')"
          fi
          echo "::endgroup::"

      - name: Save job DB cache
        id: save-cache
        if: success()
        uses: actions/cache/save@v4
        with:
          path: data
          key: gold-gem-jobs-db

      - name: Cache save summary
        if: always()
        run: |
          echo "::group::Cache summary"
          echo "Restore: cache-hit=${{ steps.restore-cache.outputs.cache-hit }}, matched-key=${{ steps.restore-cache.outputs.cache-matched-key }}"
          echo "Save: outcome=${{ steps.save-cache.outcome }}, key=gold-gem-jobs-db"
          echo "::endgroup::"
